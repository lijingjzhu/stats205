---
title: "STATS 205: The Guardian Police Killings Data Cleaning"
author: "Julie Zhu and Emily Alsentzer"
date: "April 25, 2016"
output:
  html_document:
    toc: true
    number_sections: true
    theme: spacelab
    tango: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

The goal of this study is to pre-process our `The Counted` data and remove any 
errors. After we vet the data (doing a quick sanity check), we can begin our 
analysis. We first look at some quick distributions of our variables 
to understand the spread of our data before we start performing tests and 
answering our research questions. We also try to combine some factors of
our categorical data into more interpretable buckets (matching Census
buckets).

We will first load in our libraries and our data. We obtain our gender and 
population data from the census: https://www.census.gov/popest/data/index.html

In the data, we have:

* Removed 1 gender non-conforming victim
* Combined Asian victims with Pacific Islander victims
* Combined Arab-American victims with Other victims

For our data, we used only `The Guardian` data from 2015. 

```{r}
# Libraries
library(ggplot2)
library(purrr)
library(readr)
library(lubridate)
library(tidyr)
library(dplyr)
library(stringr)
library(datasets)

# Parameters
file_out <- "data/guardian_police.csv"
counted_file <- "data/counted_2015.csv"
population_file <- "data/population.csv"
race_file <- "data/race_and_ethnicity.csv"

# Reading in data
counted <- read_csv(counted_file)
population <- read_csv(population_file, col_names = TRUE)
ethnicity <- read_csv(race_file, col_names = TRUE)
```

# Understanding the Data

We will first take a look at `The Guardian`'s police killings dataset by itself 
before we join with census data. We want to understand the missing values as 
well as the size of the data. 

The dataset has 1,145 observations and 14 variables. 

```{r}
dim(counted)
```

Looking at the summary, we see at the `uid` column doesn't quite match up with 
the number of observations we have. Name is a factor (which makes sense), but 
so is age, which is due to the fact that some ages are unknown.

We also see that the data is only from the year 2015, which is what we wanted.

```{r}
summary(counted)
```

We first introduce null values to the known ages and then convert the rest of 
the ages to a number.

```{r}
counted <- counted %>%
  mutate(age = ifelse(age == "Unknown", NA, as.numeric(age)))
```

Next, we try to understand what the other columns are. The `armed` column 
describes whether or not the deceased was armed during police confrontation. 
There are many factors to this variable including `No`, `Disputed`, and 
`Firearm`. So the predictor also lists what type of "armed" item they had. 
We have an unknown variable as well. We might want to replace this later. 

```{r}
levels(factor(counted$armed))
```

`Classification` denotes how the victim died. These include gunshot, death 
in custody (though not specified how), and tasering. We have 5 levels here. No unknowns.

```{r}
levels(factor(counted$classification))
```

## Filtering Gender

Gender comes in three categories: male, female, and non-conforming. We see that 
most of the killed are men, but there is only 1 non-conforming victim. We will 
remove this victim due to its small sample size and its inability to have any 
concrete analysis done on the specific group.

```{r}
table(counted$gender)
```

We will just quickly filter that out. 

```{r}
counted <- counted %>%
  filter(gender != "Non-conforming")
```

## Null Values

We also want to explore null values in the data set and perhaps determine if 
there are any patterns. First we set all unknown values to null, since the 
null isn't already included in the data set.

```{r}
counted[counted == "Unknown"] <- NA
```

We see that there are `r sum(is.na(counted))` total null values in the data. 
We know that a few of these came from age. 

```{r}
sum(is.na(counted))
```

We will keep these null values just in case there is matching data from 
the Washington Post data set.

We now end up with `r nrow(counted)` rows with `r ncol(counted)` columns. 

# Descriptive Statistics

In this section, we hope to explore the distribution of some variables by 
visualizing them using plots.

We obtain our census data for normalization from `https://www.census.gov/` and 
our ethnicity data from `http://kff.org/other/state-indicator/distribution-by-raceethnicity/`.

## Age

First, we look at age. We make a simple histogram and we see that the 
distribution of ages is somewhat right-skewed. We have a couple of minors 
killed by police and some elderly. Most police killing victims are middle aged. 

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(age)) %>%
  ggplot() +
  geom_histogram(mapping = aes(x = age), binwidth = 5) +
  labs(x = "Age", y = "Count")
```

## Gender

Before normalization, we see that there is an extreme gender disparity in 
those killed by the police. Men are overwhelming killed more so than women. In
the united states, there is almost an even 50-50 split of women versus men (49.8
versus 50.2). We clearly don't see this as the case here.

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(gender)) %>%
  ggplot() +
  geom_bar(mapping = aes(x = gender)) +
  labs(x = "Gender", y = "Count")
```

## Ethnicity

In order to conform to census data, we will classify `Arab-American` victims 
with the `Other` victims. 

```{r}
counted <- counted %>%
  mutate(raceethnicity = ifelse(raceethnicity == "Arab-American", 
                                "Other", raceethnicity))
```

Going off of raw counts, we see that there are more white and black victims 
than the rest of the ethnicities combined. 

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(raceethnicity)) %>%
  count(raceethnicity) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(raceethnicity, -n), y = n), 
           stat = "identity") +
  labs(x = "Ethnicity", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## Normalized Ethnicity

Normalizing for the ethnic breakdown in the population, we get a result that 
is quite different. We first wrangle the ethnicity data so that we have nicer 
race names and the like. We also combine Asian and Pacific Islander in our 
ethnicity data so that it matches the race and ethinicites we have in our `police_killings` data.

```{r, fig.height = 4, fig.width = 4}
total_pop <- ethnicity[2, 4] %>% as.numeric()
ethnicity <- ethnicity[,c(3, seq(6, 26, by = 2))] 

race_names <- c("state", "Total", "White", "Black", "Native American", "Asian", "Pacific Islander", "Other", "Other1", "Other2", "Other3", "Hispanic/Latino")
colnames(ethnicity) <- race_names

ethnicity <- ethnicity[-1,]

total_race <- ethnicity %>%
  dmap_at(race_names[2:length(race_names)], as.numeric) %>%
  mutate(county = lapply(str_split(state, ","), '[[', 1) %>% unlist(),
         state = lapply(str_split(state, ","), '[[', 2) %>% unlist(),
         Other = Other + Other1 + Other2 + Other3,
         `Asian/Pacific Islander` = Asian + `Pacific Islander`,
         county = tolower(county),
         state = trimws(state)) %>%
  select(-c(Other1, Other2, Other3, Asian, `Pacific Islander`)) %>%
  group_by(state) %>%
  summarise(White = sum(White),
            Black = sum(Black),
            `Native American` = sum(`Native American`),
            `Asian/Pacific Islander` = sum(`Asian/Pacific Islander`),
            Other = sum(Other),
            `Hispanic/Latino` = sum(`Hispanic/Latino`)) %>%
  ungroup() %>%
  gather(race, pop, White:`Hispanic/Latino`) %>%
  group_by(race) %>%
  summarise(pop = sum(pop))
```

We also have no census data for `Arab-American`, so we will group that in with 
other. 

```{r}
counted <- counted %>%
  mutate(raceethnicity = ifelse(raceethnicity == "Arab-American", 
                                "Other", raceethnicity))
```

After normalizing for ethnicity, we see that the proportion of black victims 
killed is almost twice as if they were to randomly sample people off the street. Hispanic and Native American proportions were very similar to their true 
proportion in the United States. We see that White and Asian victims are less 
likely to be killed. 

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(raceethnicity)) %>%
  count(raceethnicity) %>%
  left_join(total_race, by = c("raceethnicity" = "race")) %>%
  mutate(prop = (n/pop)*100000) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(raceethnicity, -prop), y = prop), 
           stat = "identity") +
  labs(x = "Ethnicity", y = "Number of Ethnic Race Killed per 100,000") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1)) 
```

## Armed

Next, we examine whether the victims were armed. Firearms are the most common 
form of armed victims. However, interestingly enough, we see that there are 
also a lot of unarmed victims that were shot as well. 

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(armed)) %>%
  count(armed) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(armed, -n), y = n), 
           stat = "identity") +
  labs(x = "Armed", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## Classification

Most of the victims that the police killed were killed by a gunshot. Tasering, 
death in custody, and other causes are a lot less likely.

```{r, fig.height = 4, fig.width = 4}
counted %>%
  filter(!is.na(classification)) %>%
  count(classification) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(classification, -n), y = n), 
           stat = "identity") +
  labs(x = "Death Classification", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## State

Looking at state, we see that with raw deaths, California takes the cake with 
over 200 police killings. Rounding out the top 5 we have California, Texas, 
Florida, Arizona, and Georgia. 

```{r, fig.height = 4, fig.width = 4}
counted %>%
  count(state) %>%
  top_n(10) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(state, -n), y = n), 
           stat = "identity") +
  labs(x = "State", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Normalized by Population State

Once we normalize the number of people killed, we see that the state rankings 
are quite different. 

We first wrangle the `state` data into a useable form. 

```{r}
data(state)

state_info <- data_frame(state_name = state.name,
                    state_abb = state.abb)
colnames(population) <- c(seq(1, ncol(population)))

population <- population[9:59, ] %>%
  select(1, 9) %>%
  rename(state = `1`,
         pop = `9`) %>%
  mutate(state = str_replace(state, ".", "")) %>%
  left_join(state_info, by = c("state" = "state_name")) %>%
  select(-state)
```

We filter out DC since we don't have a population estimate for DC. Rounding 
out the top five, we have Oklahoma, Arizona, Louisiana, Colorado, and 
California. 

A potential route to go here is to see if the ranking of the states is 
statistically significant from the ranking of the states by proportion of 
Black populations. 

```{r, fig.height = 4, fig.width = 5}
counted %>%
  count(state) %>%
  top_n(10) %>%
  left_join(population, by = c("state" = "state_abb")) %>%
  filter(!(state == "DC")) %>%
  mutate(pop_pct = n/pop*100000) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(state, -pop_pct), y = pop_pct), 
           stat = "identity") +
  labs(x = "State", y = "Number of State Killed per 100,000") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Month

Lastly, we look at the time of the killings. We first order the factors (which 
are months) so that we can track the number of killings over time. 

```{r}
counted$month <- ordered(counted$month, 
                                 levels = c("January", "February",
                                            "March", "April", 
                                            "May", "June", 
                                            "July,", "August", "September", 
                                            "October", "November", 
                                            "December"))
```

We see that there are the most killings in March. There seems to be a huge 
decline until June, which is the month with the lowest number of police 
killings. It locally peaks again in August before dipping for the year. We can 
test again if these cyclical patterns are statistically significant. 

```{r, fig.height = 4, fig.width = 7}
counted %>%
  filter(!is.na(month)) %>%
  count(month) %>%
  ggplot() +
  geom_line(mapping = aes(x = month, y = n), stat = "identity",
            group = 1) +
  labs(x = "Month", y = "Count") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

# Next Steps 

Next steps would be to proceed to perform the tests listed in our project 
proposal. We want to test the significances and the influences of different 
levels of each factors (in comparison to other factors). We see some potential 
with the geographic location data and the time series data. Hopefully, we can 
expand on these suggestions later.  

## Date Conversion

Lastly, we turn the three date features (day, month, year) into a single date 
so it is easier to analyze. 

```{r}
counted <- counted %>%
  mutate(month = ifelse(month == "January", 1, month),
         month = ifelse(month == "February", 2, month),
         month = ifelse(month == "March", 3, month),
         month = ifelse(month == "April", 4, month),
         month = ifelse(month == "May", 5, month),
         month = ifelse(month == "June", 6, month),
         month = ifelse(month == "July", 7, month),
         month = ifelse(month == "August", 8, month),
         month = ifelse(month == "September", 9, month),
         month = ifelse(month == "October", 10, month),
         month = ifelse(month == "November", 11, month),
         month = ifelse(month == "December", 12, month)) %>%
  mutate(date = ymd(str_c(year, month, day, sep = "-"))) %>%
  rename(race = raceethnicity) %>%
  select(-c(month, day, year)) %>%
  dmap_at("date", ymd)
```

As an after thought, we also renamed the column `raceethnicity` to `race` so 
we can deal with shorter variable names. Yay. 

## Writing Out Data

Finally, we can write out our edited data to a new file so that we can load 
it in quickly for our next study. 

```{r}
write_csv(x = counted, path = file_out)
```