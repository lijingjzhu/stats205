---
title: "STATS 205: Police Killings Data Validation"
author: "Julie Zhu and Emily Alsentzer"
date: "April 25, 2016"
output:
  html_document:
    toc: true
    number_sections: true
    theme: spacelab
    tango: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

The goal of this study is to pre-process our data and remove any errors. After we vet the data (removing rows, fixing errors, inputing missing values), we can begin our analysis. We first look at some quick distributions of our variables to understand the spread of our data before we start performing tests and answering our research questions. 

We will first load in our libraries and our data. We obtain our gender and population data from the census: https://www.census.gov/popest/data/index.html

In the data, we have:

* Removed NAs
* Removed 1 gender non-conforming victim
* Combined Asian victims with Pacific Islander victims
* Combined Arab-American victims with Other victims

```{r}
# Libraries
library(ggplot2)
library(purrr)
library(readr)
library(lubridate)
library(tidyr)
library(dplyr)
library(stringr)
library(datasets)

# Parameters
file_out <- "police_killings.csv"
police_file <- "counted_2015.csv"
gender_file <- "gender.csv"
population_file <- "population.csv"
race_file <- "ethnicity.csv"
```

```{r cars}
police_killings <- read_csv(police_file)
gender <- read_csv(gender_file, col_names = TRUE)
population <- read_csv(population_file, col_names = TRUE)
ethnicity <- read_csv(race_file, col_names = FALSE)
```

# Understanding the Data

We will first take a look at `The Guardian`'s police killings dataset by itself before we join with census data. We want to understand the missing values as well as the size of the data. 

The dataset has 1,145 observations and 14 variables. 

```{r}
dim(police_killings)
```

Looking at the summary, we see at the `uid` column doesn't quite match up with the number of observations we have. Name is a factor (which makes sense), but so is age, which is due to the fact that some ages are unknown.

We also see that the data is only from the year 2015, which is what we wanted.

```{r}
summary(police_killings)
```

We first introduce null values to the known ages and then convert the rest of the ages to a number.

```{r}
police_killings <- police_killings %>%
  mutate(age = ifelse(age == "Unknown", NA, as.numeric(age)))
```

Next, we try to understand what the other columns are. The `armed` column describes whether or not the deceased was armed during police confrontation. There are many factors to this variable including `No`, `Disputed`, and `Firearm`. So the predictor also lists what type of "armed" item they had. We have an unknown variable as well. We might want to replace this later. 

```{r}
levels(factor(police_killings$armed))
```

`Classification` denotes how the victim died. These include gunshot, death in custody (though not specified how), and tasering. We have 5 levels here. No unknowns.

```{r}
levels(factor(police_killings$classification))
```

## Filtering Gender

Gender comes in three categories: male, female, and non-conforming. We see that most of the killed are men, but there is only 1 non-conforming victim. We will remove this victim due to its small sample size and its inability to have any concrete analysis done on the specific group.

```{r}
table(police_killings$gender)
```

We will just quickly filter that out. 

```{r}
police_killings <- police_killings %>%
  filter(!(gender == "Non-conforming"))
```

## Null Values

We also want to explore null values in the data set and perhaps determine if there are any patterns. First we set all unknown values to null, since the null isn't already included in the data set.

```{r}
police_killings[police_killings == "Unknown"] <- NA
```

We see that there are 92 total null values in the data. We know that 4 of these came from age. 

```{r}
sum(is.na(police_killings))
```

Since the number of null values in the data set is less than 10% of the data, we decided to remove these rows.

```{r}
police_killings <- police_killings %>%
  na.omit()
```

We now end up with 1,060 rows with 14 columns. 

# Descriptive Statistics

In this section, we hope to explore the distribution of some variables by visualizing them using plots.

We obtain our census data for normalization from `https://www.census.gov/` and our ethnicity data from `http://kff.org/other/state-indicator/distribution-by-raceethnicity/`.

## Age

First, we look at age. We make a simple histogram and we see that the distribution of ages is somewhat right-skewed. We have a couple of minors killed by police and some elderly. Most police killing victims are middle aged. 

```{r}
police_killings %>%
  ggplot() +
  geom_histogram(mapping = aes(x = age), binwidth = 5) +
  labs(x = "Age", y = "Count")
```

## Gender

Before normalization, we see that there is an extreme gender disparity in those killed by the police. Men are overwhelming killed more so than women. 

```{r}
police_killings %>%
  ggplot() +
  geom_bar(mapping = aes(x = gender)) +
  labs(x = "Gender", y = "Count")
```

## Normalized Gender

Even with we normalize for gender (which is a near 50-50 split), we see the same gender disparity in people killed by the police. 

In order to see this visually, we need to wrangle some data and transform the census gender data for 2015. 

```{r}
population_vars <- c("total_pop", "male_pop", "female_pop")

gender <- gender[2:nrow(gender), 3:6] %>%
  rename(state = `GEO.display-label`,
         total_pop = `cen42010sex0_age999`,
         male_pop = `cen42010sex1_age999`,
         female_pop = `cen42010sex2_age999`) %>%
  dmap_at(population_vars, as.numeric) %>%
  mutate(female_pct = female_pop/total_pop,
         male_pct = male_pop/total_pop)
```

We have a line indicating if women or men were killed equally by the police. This represents the ratio of the sample proportion of females and males killed by the police over the actual proportion of females and males in the United States. 

```{r}
gender_norm <- gender %>% 
  filter(state == "United States") %>%
  gather(gender_pct, pct_pop, female_pct:male_pct) %>%
  select(gender_pct, pct_pop) %>%
  rename(gender = gender_pct) %>%
  mutate(gender = ifelse(gender == "female_pct", "Female", "Male"))
  
police_killings %>%
  count(gender) %>%
  left_join(gender_norm, by = c("gender" = "gender")) %>%
  mutate(prop = n/nrow(police_killings),
         count = prop/pct_pop) %>%
  ggplot() +
  geom_bar(mapping = aes(x = gender, y = count), stat = "identity") +
  labs(x = "Gender", y = "Percent Increase in Gender Killed") + 
  geom_hline(yintercept = 1)
```

## Ethnicity

In order to conform to census data, we will classify `Arab-American` victims with the `Other` victims. 
```{r}
police_killings <- police_killings %>%
  mutate(raceethnicity = ifelse(raceethnicity == "Arab-American", 
                                "Other", raceethnicity))
```

Going off of raw counts, we see that there are more white and black victims than the rest of the ethnicities combined. 

```{r}
police_killings %>%
  count(raceethnicity) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(raceethnicity, -n), y = n), 
           stat = "identity") +
  labs(x = "Ethnicity", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## Normalized Ethnicity

Normalizing for the ethnic breakdown in the population, we get a result that is quite different. We first wrangle the ethnicity data so that we have nicer race names and the like. We also combine Asian and Pacific Islander in our ethnicity data so that it matches the race and ethinicites we have in our `police_killings` data.

```{r}
ethnicity <- ethnicity[seq(22, 35, by = 2), 1:2] %>%
  rename(raceethnicity = X1,
         pct = X2)

race_names <- c("Black", "Native American", "Asian", "Pacific Islander", "Other", "Hispanic/Latino", "White")
ethnicity$raceethnicity <- race_names

add_together <- c("Asian", "Pacific Islander")

ethnicity <- ethnicity %>%
  dmap_at("pct", as.numeric) %>%
  mutate(raceethnicity = ifelse(raceethnicity %in% add_together, 
                                "Asian/Pacific Islander", raceethnicity)) %>%
  group_by(raceethnicity) %>%
  summarise(pop_pct = sum(pct)/100)
```

We also have no census data for `Arab-American`, so we will group that in with other. 

```{r}
police_killings <- police_killings %>%
  mutate(raceethnicity = ifelse(raceethnicity == "Arab-American", 
                                "Other", raceethnicity))
```

After normalizing for ethnicity, we see that the proportion of black victims killed is almost twice as if they were to randomly sample people off the street. Hispanic and Native American proportions were very similar to their true proportion in the United States. We see that White and Asian victims are less likely to be killed. 

```{r}
police_killings %>%
  count(raceethnicity) %>%
  left_join(ethnicity, by = c("raceethnicity" = "raceethnicity")) %>%
  mutate(race_pct = (n/nrow(police_killings)),
         change = race_pct/pop_pct) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(raceethnicity, -change), y = change), 
           stat = "identity") +
  labs(x = "Ethnicity", y = "Percent Increase in Killings of Ethnicity") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1)) + 
  geom_hline(yintercept = 1)
```

## Armed

Next, we examine whether the victims were armed. Firearms are the most common form of armed victims. However, interestingly enough, we see that there are also a lot of unarmed victims that were shot as well. 

```{r}
police_killings %>%
  count(armed) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(armed, -n), y = n), 
           stat = "identity") +
  labs(x = "Armed", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## Classification

Most of the victims that the police killed were killed by a gunshot. Tasering, death in custody, and other causes are a lot less likely.

```{r}
police_killings %>%
  count(classification) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(classification, -n), y = n), 
           stat = "identity") +
  labs(x = "Death Classification", y = "Count") +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

## State

Looking at state, we see that with raw deaths, California takes the cake with over 150 police killings. 

```{r}
police_killings %>%
  count(state) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(state, -n), y = n), 
           stat = "identity") +
  labs(x = "State", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Normalized by Population State

Once we normalize the number of people killed, we see that the state rankings are quite different. 

We first wrangle the `state` data into a useable form. 

```{r}
data(state)

state_info <- data_frame(state_name = state.name,
                    state_abb = state.abb)

colnames(population) <- c(seq(1, ncol(population)))

population <- population[9:59, ] %>%
  select(1, 9) %>%
  rename(state = `1`,
         pop = `9`) %>%
  mutate(state = str_replace(state, ".", "")) %>%
  left_join(state_info, by = c("state" = "state_name")) %>%
  select(-state)
```

We filter out DC since we don't have a population estimate for DC. We now see that New Mexico, Okaloma, and Wyoming have the highest police killings rate than any other state (besides DC, since we don't have information for them). 

A potential route to go here is to see if the ranking of the states is statistically significant from the ranking of the states by proportion of Black populations. 

```{r}
police_killings %>%
  count(state) %>%
  left_join(population, by = c("state" = "state_abb")) %>%
  filter(!(state == "DC")) %>%
  mutate(pop_pct = n/pop) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(state, -pop_pct), y = pop_pct), 
           stat = "identity") +
  labs(x = "State", y = "Percent of State Population") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Month

Lastly, we look at the time of the killings. We first order the factors (which are months) so that we can track the number of killings over time. 

```{r}
police_killings$month <- ordered(police_killings$month, 
                                 levels = c("January", "February",
                                            "March", "April", 
                                            "May", "June", 
                                            "July,", "August", "September", 
                                            "October", "November", 
                                            "December"))
```

We see that there are the most killings in March. There seems to be a huge decline until June, which is the month with the lowest number of police killings. It locally peaks again in August before dipping for the year. We can test again if these cyclical patterns are statistically significant. 

```{r}
police_killings %>%
  filter(!is.na(month)) %>%
  count(month) %>%
  ggplot() +
  geom_line(mapping = aes(x = month, y = n), stat = "identity",
            group = 1) +
  labs(x = "Month", y = "Count") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

# Next Steps 

Next steps would be to proceed to perform the tests listed in our project proposal. We want to test the significances and the influences of different levels of each factors (in comparison to other factors). We see some potential with the geographic location data and the time series data. Hopefully, we can expand on these suggestions later.  

## Writing Out Data

Finally, we can write out our edited data to a new file so that we can load it in quickly for our next study. 

```{r}
write.csv(x = police_killings, file = file_out)
```